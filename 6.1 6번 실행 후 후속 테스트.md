## yaml review
```
apiVersion: v1                       # 이 리소스의 API 그룹/버전. Namespace는 core(v1)에 속함.
kind: Namespace                      # 만들 리소스 종류: 네임스페이스(논리적 격리 단위)
metadata:                            # 리소스 메타정보(이름/라벨/어노테이션 등)
  name: demo-hpa                     # 네임스페이스 이름. 이후 리소스들이 이 공간 안에 생성됨.

---
apiVersion: apps/v1                  # Deployment는 apps API 그룹의 v1을 사용
kind: Deployment                     # 만들 리소스 종류: Deployment (ReplicaSet을 통해 Pod 복제/롤링업데이트 관리)
metadata:
  name: nginx                        # Deployment 이름(네임스페이스 내 유일)
  namespace: demo-hpa                # 이 Deployment가 생성될 네임스페이스 지정 (없으면 default)
spec:                                # Deployment의 “원하는 상태(desired state)” 정의
  replicas: 2                        # 원하는 Pod 개수(초기값). HPA가 붙으면 이 값은 “기본”이고 실제는 HPA가 조정.
  selector:                          # 이 Deployment가 어떤 Pod를 “자기 것”으로 관리할지 선택 규칙
    matchLabels:                     # 라벨 매칭 방식(가장 흔함)
      app: nginx                     # 라벨 app=nginx 인 Pod를 이 Deployment가 관리(ReplicaSet 포함)
  template:                          # Deployment가 생성할 Pod의 템플릿(설계도)
    metadata:
      labels:                        # Pod에 붙일 라벨(서비스 셀렉터/디플로이 셀렉터와 일치해야 함)
        app: nginx                   # Pod 라벨. 위 selector.matchLabels 와 반드시 논리적으로 일치해야 함(불일치 시 오류/예상치 못한 동작).
    spec:                            # Pod 스펙(컨테이너/볼륨/노드스케줄링 등)
      containers:                    # Pod 안에서 실행될 컨테이너 목록(대부분 1개, sidecar면 여러 개)
      - name: nginx                  # 컨테이너 이름(해당 Pod 내에서 유일)
        image: nginx:1.27-alpine     # 사용할 컨테이너 이미지(태그 포함). Alpine 기반이라 가볍고 빠름.
        ports:
        - containerPort: 80          # 컨테이너 내부에서 listen 하는 포트 “정보”. (Service의 targetPort와 연결될 가능성이 큼)
                                   # 주의: 이 설정만으로 방화벽/노출이 되는 건 아니고, “메타 정보”에 가까움.

        # HPA는 requests를 기준으로 CPU Utilization(%) 계산
        resources:                   # 리소스 요청/제한을 설정(스케줄링/제한/오토스케일 기준에 영향)
          requests:                  # "요청(request)" = 이 컨테이너가 최소 이만큼은 필요하다고 선언
            cpu: "50m"               # 50 millicore = 0.05 vCPU. 
                                   # 스케줄러는 노드의 allocatable에서 requests 합을 보고 배치함.
                                   # HPA의 CPU Utilization(%) 계산에서 분모가 되는 기준이 됨(아래 설명).
            memory: "64Mi"           # 메모리 최소 요구량 64MiB. 스케줄링 시 이만큼 자리 필요.
          limits:                    # "제한(limit)" = 컨테이너가 최대 이 이상 쓰지 못하게 제한(강제)
            cpu: "300m"              # CPU 최대 0.3 vCPU 정도로 제한. 초과 사용 시 throttling(쓰로틀링) 발생.
            memory: "128Mi"          # 메모리 최대 128MiB. 초과 시 OOMKilled(강제 종료)될 수 있음.

---
apiVersion: v1                       # Service는 core(v1)
kind: Service                        # 만들 리소스 종류: Service (Pod 집합에 대한 안정적 가상 IP/이름 제공)
metadata:
  name: nginx-svc                    # 서비스 이름 (DNS: nginx-svc.demo-hpa.svc.cluster.local)
  namespace: demo-hpa                # 서비스가 속할 네임스페이스
spec:
  selector:                          # 어떤 Pod들을 이 서비스의 엔드포인트로 묶을지
    app: nginx                       # 라벨 app=nginx 인 Pod들이 대상(Deployment가 만든 Pod와 일치)
  ports:
  - port: 80                         # Service가 클러스터 내부에서 제공하는 포트(클라이언트는 이 포트로 접속)
    targetPort: 80                   # 실제 Pod(컨테이너)로 전달되는 포트(보통 containerPort와 동일)
                                   # (명시 안 하면 port와 동일하다고 간주)

---
apiVersion: autoscaling/v2           # HPA v2 API. behavior, multiple metrics 등 고급 설정 가능.
kind: HorizontalPodAutoscaler        # 만들 리소스 종류: HPA (수평 확장: Pod 개수 조절)
metadata:
  name: nginx-hpa                    # HPA 이름
  namespace: demo-hpa                # HPA가 적용될 네임스페이스(대상 Deployment와 동일해야 함)
spec:
  scaleTargetRef:                    # “어떤 대상”의 replicas를 조절할지 참조
    apiVersion: apps/v1              # 대상 리소스의 API 버전
    kind: Deployment                 # 대상 리소스 종류
    name: nginx                      # 대상 리소스 이름(= 위 Deployment nginx)
  minReplicas: 2                     # 최소 Pod 개수. replicas가 이보다 내려가지 않게 보장(가용성/HA 관점)
  maxReplicas: 4                     # 최대 Pod 개수. 무한 확장 방지(비용/안정성)
  metrics:                           # 오토스케일 판단에 사용할 지표 목록(복수 가능)
  - type: Resource                   # 리소스 기반 지표(CPU/Memory 등). (Pod 지표, External 지표 등도 가능)
    resource:
      name: cpu                      # 측정할 리소스: cpu
      target:
        type: Utilization            # “사용률(%)” 기반 목표
        averageUtilization: 50       # 목표: 평균 CPU 사용률을 50%로 맞추도록 Pod 수 조절
                                   # 여기서 50%는 "requests.cpu"를 기준으로 계산됨.
                                   # 예) requests=50m 인 Pod가 실제 25m 쓰면 utilization=50%
                                   # 예) 60m 쓰면 utilization=120% (요청치보다 많이 쓰는 상태)

  behavior:                          # 스케일 업/다운의 “속도/안정화” 정책 (v2의 핵심 고급 기능)
    scaleUp:                         # Pod를 늘릴 때(확장) 정책
      stabilizationWindowSeconds: 0  # 확장 시 “안정화 창” 0초 = 즉시 반영(가장 공격적인 확장)
                                   # (안정화 창은 최근 권고치들을 보고 급격한 변동을 완화)
      policies:                      # 확장 속도를 제한하는 규칙들(여러 개 가능)
      - type: Pods                   # 절대값 기준 정책
        value: 2                     # 한 번의 스케일 이벤트에서 최대 +2 Pod까지 증가 허용
        periodSeconds: 15            # 이 정책이 적용되는 기간(15초 단위로 평가)
      - type: Percent                # 비율 기준 정책
        value: 100                   # 한 번의 스케일 이벤트에서 현재 replicas의 최대 100%까지 증가 허용
                                   # 예) 현재 2개면 +2까지(=100% 증가)
                                   # 예) 현재 3개면 +3까지 허용하지만 maxReplicas=4 때문에 실제는 +1까지만 가능
        periodSeconds: 15            # 15초 단위
      selectPolicy: Max              # 여러 policies가 동시에 있을 때 “더 크게 늘릴 수 있는” 정책을 선택
                                   # 즉, Pods(+2) vs Percent(+100%) 중 더 큰 증가폭 허용을 채택.
                                   # 이 설정은 “부하 급증에 빨리 대응”하려는 공격적 확장 성향.
```
### apps라는 ‘API 그룹(API Group)’**이 Kubernetes에 있고, 그 그룹 안에 Deployment 같은 리소스 타입(kind) 이 들어있다는 뜻이에요.
```
정리하면:

apiVersion: apps/v1

apps = API 그룹 이름

v1 = 그 API 그룹의 버전

kind: Deployment

Deployment라는 리소스 타입(Kind) 을 만들겠다

즉, Kubernetes가 제공하는 건 이렇게 계층이 있어요:

1) API 그룹 / 버전

v1 (core group) : Namespace, Pod, Service, ConfigMap 같은 것들

core는 그룹명이 생략돼서 그냥 v1처럼 보임

apps/v1 : Deployment, ReplicaSet, StatefulSet, DaemonSet 등

batch/v1 : Job, CronJob

networking.k8s.io/v1 : Ingress, NetworkPolicy

rbac.authorization.k8s.io/v1 : Role, ClusterRole 등

2) 리소스 타입(Kind)

각 그룹/버전 아래에 실제 “종류”가 존재합니다.

예: apps/v1 아래에 Deployment, ReplicaSet, StatefulSet…

왜 이렇게 나눠놨나?

기능 영역별로 API를 묶고(확장/관리 쉬움)

버전(v1, v1beta1 등)로 안정성/호환성 관리

CRD 같은 확장도 “그룹/버전” 형태로 추가 가능
```
---
```
ubuntu@cp1:~$ kubectl api-resources | head
NAME                                SHORTNAMES   APIVERSION                          NAMESPACED   KIND
bindings                                         v1                                  true         Binding
componentstatuses                   cs           v1                                  false        ComponentStatus
configmaps                          cm           v1                                  true         ConfigMap
endpoints                           ep           v1                                  true         Endpoints
events                              ev           v1                                  true         Event
limitranges                         limits       v1                                  true         LimitRange
namespaces                          ns           v1                                  false        Namespace
nodes                               no           v1                                  false        Node
persistentvolumeclaims              pvc          v1                                  true         PersistentVolumeClaim


ubuntu@cp1:~$ kubectl api-resources | grep -i deployment
deployments                         deploy       apps/v1                             true         Deployment
ubuntu@cp1:~$
```

---
---


## 워커노드에서도 pod 목록 보기

![alt text](image-16.png)

## k3s 환경 정리 메모 (Ingress + Metrics Server + Kubernetes Dashboard + HPA/노드 확인)

> 이 문서는 대화에서 나온 내용을 한 번에 보기 좋게 정리한 것입니다. (k3s 기준)

---

## 1) k3s에서 Ingress + Metrics Server + Kubernetes Dashboard “mesh up” 관점

### 레이어(역할)로 나누어 이해
- **Edge Ingress 레이어 (외부 → 클러스터)**  
  - 예: **Traefik**(k3s 기본 인그레스)  
  - 외부 HTTP(S) 요청을 받아 **클러스터 Service로 라우팅**
- **App Gateway/Proxy 레이어 (앱 전용 프록시)**  
  - 예: Dashboard Helm chart가 설치하는 **kubernetes-dashboard-kong-proxy**  
  - “클러스터 공용 인그레스 컨트롤러”가 아니라 **Dashboard 앱 앞단 전용 게이트웨이**
- **Observability/Telemetry 레이어 (메트릭 파이프)**  
  - 예: **metrics-server**  
  - kubelet → metrics-server → `metrics.k8s.io` 제공 (`kubectl top`, Dashboard 리소스 그래프에 필요)
- **Dashboard App 레이어**  
  - Dashboard UI 및 관련 컴포넌트들(예: metrics-scraper 등)

### 권장 트래픽 흐름(대표 패턴)
- **Traefik → Service(kong-proxy) → dashboard 내부 서비스**
  - 외부 진입은 Traefik 하나로 통합
  - Dashboard 내부 구조 변경(차트 업그레이드)에도 Ingress 대상은 kong-proxy로 유지 가능

### Helm의 역할
- Helm은 “트래픽 메쉬”가 아니라 **배포/업그레이드/롤백 도구**
- 실제 연결은 Helm이 만든 리소스를 **Ingress(또는 Traefik CRD)**로 “엮어서” 완성

---

## 2) `kubectl apply -f -` 로 적용한 YAML은 worker node에 반영되는가?

### 핵심 개념
- `kubectl apply`로 만든 오브젝트(Deployment/Service/HPA 등)는 **API Server에 저장되는 desired state**
- “노드에 반영”은 그 결과 생성되는 **Pod가 어느 노드에 스케줄링/실행되었는지**로 확인

### 확인 방법(가장 빠름)
```bash
kubectl -n demo-hpa get deploy,rs,pod,svc,hpa -o wide
kubectl -n demo-hpa get pod -o wide
kubectl -n demo-hpa describe pod -l app=nginx
```

---

## 3) 왜 w1만 보이고 w2는 안 쓰였나?

### 결론
- **replicas=2**이면 Pod가 2개뿐이라, 스케줄러가 **cp1 1개 + w1 1개**로 배치할 수 있음
- w2에 “미반영”이 아니라 **배치할 Pod 수가 적어 w2까지 갈 일이 없었던 것**일 가능성이 큼

### w2가 스케줄링 대상인지 체크
```bash
kubectl get nodes -o wide
kubectl describe node w2 | egrep -n "Unschedulable|SchedulingDisabled|Taints"
```
- `Unschedulable: true`면 `kubectl uncordon w2`
- `Taints: ... NoSchedule`가 있으면 일반 Pod가 못 감

### w2에 뜨는 걸 확인하려면
- replica를 늘리기:
```bash
kubectl -n demo-hpa scale deploy/nginx --replicas=3
kubectl -n demo-hpa get pod -o wide
```

---

## 4) vi에서 클립보드 내용을 “깨지지 않게” 정확히 붙여넣기

### 가장 안전: paste 모드
1) `Esc` → `:set paste` → Enter  
2) `i` (입력) → 붙여넣기  
3) 완료 후 `Esc` → `:set nopaste` → Enter

상태 확인:
```vim
:set paste?
```

### 더 안전(vi 자체를 피하고 파일에 그대로 append)
```bash
sudo tee -a /etc/hosts >/dev/null <<'EOF'
192.168.56.10 cp1
192.168.56.11 w1
192.168.56.12 w2
EOF
```

---

## 5) `kubectl apply` 에러 정리

### 5.1 `kubectl apply ./deploy.yaml`
- `apply`는 파일을 직접 인자로 받지 않고 **`-f`가 필요**
```bash
kubectl apply -f ./deploy.yaml
```

### 5.2 `apiVersion not set`
- 파일 내용이 K8s 매니페스트가 아니거나
- 파일 앞부분이 깨져서 첫 리소스의 `apiVersion:`을 인식 못하는 경우

#### 빠른 확인
```bash
sed -n '1,30p' ./deploy.yaml
grep -n "apiVersion" ./deploy.yaml
cat -A ./deploy.yaml | sed -n '1,30p'
```

#### 흔한 원인: BOM/CRLF 제거
```bash
sudo sed -i '1s/^\xEF\xBB\xBF//' ./deploy.yaml
sudo sed -i 's/\r$//' ./deploy.yaml
kubectl apply -f ./deploy.yaml
```

---

## 6) worker node(w1)에서 부하 상태/Pod 확인 가능?

가능합니다. 다만 두 관점이 있습니다.

### A) 클러스터 관점(권장): `kubectl`로 w1에 스케줄된 Pod만 보기
```bash
kubectl get pods -A -o wide --field-selector spec.nodeName=w1
kubectl -n demo-hpa get pods -o wide --field-selector spec.nodeName=w1
```

리소스(메트릭 서버 필요):
```bash
kubectl top node w1
kubectl top pods -A --field-selector spec.nodeName=w1
```

### B) 노드 로컬 관점(k3s agent에서도 즉시 가능): `k3s crictl`
k3s는 containerd 기반이라 Docker 대신 아래를 사용:
```bash
sudo k3s crictl pods
sudo k3s crictl ps
sudo k3s crictl stats --no-stream
sudo k3s crictl logs <CONTAINER_ID>
```

시스템 부하:
```bash
uptime
top
free -h
```

---

## 7) w1에서 `kubectl`이 localhost:8080으로 붙으며 실패하는 이유/해결

### 증상
w1에서:
```bash
kubectl get pods -A -o wide --field-selector spec.nodeName=w1
```
실행 시 `localhost:8080 refused` 발생

### 원인
- w1은 **k3s-agent** 노드이고, API Server는 cp1(server)에 있음
- w1에 올바른 kubeconfig가 없으면 `kubectl`이 기본값(=localhost:8080)으로 시도함

### 해결(권장): cp1 kubeconfig를 w1로 복사 후 server 주소를 cp1 IP로 변경
w1에서(예: cp1 IP가 192.168.56.10인 경우):
```bash
mkdir -p ~/.kube
scp ubuntu@cp1:/etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown -R $USER:$USER ~/.kube
chmod 600 ~/.kube/config

sed -i 's#https://127.0.0.1:6443#https://192.168.56.10:6443#g' ~/.kube/config
sed -i 's#https://localhost:6443#https://192.168.56.10:6443#g' ~/.kube/config

kubectl get nodes
kubectl get pods -A -o wide --field-selector spec.nodeName=w1
```

네트워크 점검(필요 시):
```bash
curl -k https://192.168.56.10:6443/livez
```

---

## 8) HPA 관련 빠른 체크 포인트

HPA 동작 조건:
- metrics-server 정상
- Pod에 `resources.requests.cpu` 설정(이미 YAML에 포함)

확인:
```bash
kubectl get apiservices | grep metrics
kubectl top nodes
kubectl -n demo-hpa top pods
kubectl -n demo-hpa describe hpa nginx-hpa
```

---

### 참고: “w1에서 당장 확인” 최단 루트
- kubeconfig 없이도 바로:
```bash
sudo k3s crictl ps
sudo k3s crictl stats --no-stream
```
- kubeconfig 세팅 후엔:
```bash
kubectl get pods -A -o wide --field-selector spec.nodeName=w1
kubectl top node w1
```
