# 2026-01-22

## 새로 vm을 만들어 k3s join하려면?

1. 새로 만든 vm 터미널에서 고정 전 ip 주소 알아보고 접속

    ```bash
    # 새로만든 utm vm 터미널에서 
    hostname -I

    # 출력예시 )  
    192.168.64.xx fd56:24b0:1e31:dd83:xxxx:yyyy:zzzz:abcd 
    
    # 이후 zzh 터미널에서 접속
    # 여기서 <username>은 내가 정한 username으로 치환 
    ssh <username>@192.168.64.xx
    ```

2. ip를 고정 (여기서는 192.168.64.13로 함)

    ```bash
    sudo tee /etc/netplan/00-installer-config.yaml >/dev/null <<'EOF'
    network:
        version: 2
        ethernets:
            enp0s1:
                dhcp4: no
                addresses: [192.168.64.13/24]
                routes:
                    - to: default
                    via: 192.168.64.1
                nameservers:
                    addresses: [1.1.1.1, 8.8.8.8]
    EOF

    sudo chmod 600 /etc/netplan/00-installer-config.yaml
    sudo mv /etc/netplan/50-cloud-init.yaml /etc/netplan/50-cloud-init.yaml.bak
    sudo netplan apply

    # 그러면 ip 고정이 완료되고 아래와 같은 메세지가 나오면서 연결이 끊긴다
    Read from remote host 192.168.64.6: Operation timed out
    Connection to 192.168.64.6 closed. 
    client_loop: send disconnect: Broken pipe


    # ip 확인
    ip -4 route show default
    ip -br a
    ```

    - `192.168.64.13/24` `/24`는 서브넷 마스크를 CIDR 표기로 적은것
    - `/24`는 255.255.255.0을 의미해서, 같은 네트워크 범위가 192.168.64.0 ~ 192.168.64.255임을 나타낸다.
    - 따라서, `192.168.64.13/24`는 “네트워크는 192.168.64.0~255 범위이고, 그 중 하나가 192.168.64.13”이라는 뜻

3. cn(여기서는 k8s-master)에서 토큰 발급

```bash
sudo cat /var/lib/rancher/k3s/server/node-token
```

- 새로 만든 vm(여기서는 k8s-w3) 에서 k3s agent join

    ```bash
    curl -sfL https://get.k3s.io | \
        K3S_URL="https://192.168.64.10:6443" \
        K3S_TOKEN="(발급받은토큰)" sh -s - \
        --node-ip 192.168.64.13 \
        --flannel-iface enp0s1

    ```

    ```bash
    # 마지막에 아래와 같은 메세지가 나오면 잘 된것
    ...
    ...
    ...
    [INFO]  systemd: Starting k3s-agent
    ```

---

### nginx nodeport + HPA

```bash
kubectl create deploy nginx --image=nginx:1.27-alpine
kubectl expose deploy nginx --type=NodePort --port 80 --name nginx-nodeport

cat <<'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "50m"
            memory: "64Mi"
          limits:
            cpu: "300m"
            memory: "128Mi"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 2
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
EOF

```

```bash
#확인
kubectl get pods -o wide
kubectl get hpa
kubectl get svc nginx-nodeport
```

---

### 시나리오 기반 실습

1. **트래픽 급증 대응**

    ```bash
    # HPA 스케일다운 모니터링
    kubectl get hpa -w
    ```

    ```bash
    # 부하테스트 도구(fortio) 임시 파드 실행 후 끝나면 자동 삭제
    kubectl run -it --rm fortio \
    --image=fortio/fortio --restart=Never -- \
    load -qps 0 -t 3m -c 200 http://nginx-nodeport/
    
    # 3분 동안 동시 200개로 nginx에 부하를 주는 테스트
    ```

    테스트 시작
    ![테스트실행](/imgs/스크린샷%202026-01-22%20오후%2012.03.44.png)
    부하가 시작되자 HPA가 레플리카를 2 → 4로 늘립니다.

    테스트종료
    ![테스트종료](/imgs/스크린샷%202026-01-22%20오후%2012.51.08.png)
    부하가 사라지면 HPA가 레플리카를 4 → 2로 줄입니다.

2. **노드 장애 대응: w2 VM 종료 → 파드 재배치 확인**

    1) 현재 파드 위치 확인 (master)

        ```bash
        kubectl get pods -o wide
        ```

    2) w2 VM 종료
    UTM에서 k8s-w2 종료(파워 오프)

    3) 노드 상태 확인 (master)

        ```bash
        kubectl get nodes
        ```

        w2가 NotReady로 바뀌는지 확인

    4) 파드 재배치 확인

        ```bash
        kubectl get pods -o wide
        ```

        쿠버네티스가 노드를 장애로 판단하는 데 시간이 걸려서 바로 재배치가 되는건 아니라고 함. 보통 몇 분 지연.
        ![테스트종료](/imgs/스크린샷%202026-01-22%20오후%203.59.28.png)
        파드가 w1에 올라가서 재배치가 완료

    5) UTM에서 k8s-w2 VM 다시 켜고 수분 뒤 `kubectl get nodes` 명령어로 node 상태를 확인해보면 k8s-w2 가 Ready 상태인 걸 확인 가능하다. 다만 자동으로 다시 파드를 할당하거나 분산시키지는 않는다. (재스케줄링은 장애나 스케일링 때만 발생)

3. **롤링 업데이트/롤백**

    (아래는 master SSH 세션에서 실행)

    1) 현재 상태 확인

        ```bash
        kubectl get deploy nginx
        kubectl get pods -l app=nginx -o wide
        ```

    2) 이미지 업데이트 (1.27 → 1.28)

        ```bash
        kubectl set image deploy/nginx nginx=nginx:1.28-alpine
        kubectl rollout status deploy/nginx
        ```

    3) 버전 확인

        ```bash
        kubectl get pods -l app=nginx -o wide
        kubectl describe deploy nginx | grep -i image
        ```

    4) 롤백

        ```bash
        kubectl rollout undo deploy/nginx
        kubectl rollout status deploy/nginx
        ```

    5) 롤백 확인

        ```bash
        kubectl describe deploy nginx | grep -i image
        ```
