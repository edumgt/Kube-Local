## 쿠버네티스 자체(원 기능) 과 쿠버네티스 위에 얹어 쓰는 대표 서드파티(애드온/에코시스템) 솔루션

1) 쿠버네티스의 원 기능(코어 기능)
A. 클러스터 기본 동작(오케스트레이션)

스케줄링: Pod를 어떤 Node에 배치할지 결정 (scheduler)

상태 유지/자가 치유: 장애 시 재시작/재스케줄 (kubelet + controller)

스케일링: Deployment/ReplicaSet 기반 수평 확장(원칙적으로 코어 오브젝트로 가능)

롤링 업데이트/롤백: Deployment 기본 동작

B. 워크로드/리소스 모델(핵심 오브젝트)

Pod, Deployment, ReplicaSet, StatefulSet, DaemonSet, Job/CronJob

ConfigMap, Secret(설정/민감정보 주입)

Namespace, Label/Selector, Annotation

C. 서비스 디스커버리 & L4 로드밸런싱

Service(ClusterIP/NodePort/LoadBalancer): 내부/외부 접근의 기본 단위

CoreDNS(일반적으로 기본 구성요소로 설치됨): 서비스 이름 기반 DNS

D. 네트워킹의 “규약”

CNI 플러그인 연동 구조(K8s는 “인터페이스/규칙” 제공, 실제 구현은 플러그인)

NetworkPolicy 오브젝트(정책 “리소스”는 코어, 실제 enforcement는 CNI가 담당)

E. 스토리지의 “규약”

Volume / PV / PVC / StorageClass

CSI 연동 구조(K8s는 “인터페이스/규칙”, 실제 드라이버는 벤더/프로젝트)

F. 접근제어/보안(기본 프레임)

RBAC, ServiceAccount

API 인증/인가, Admission(정책 관문)

(버전/환경에 따라) Pod Security 표준(정책 기반)

G. 운영/확장 메커니즘

kubectl / API 서버 / etcd / controller-manager 등 컨트롤 플레인 구성요소

CRD(커스텀 리소스 정의): “확장하는 틀”은 코어

Operator 패턴을 담을 수 있는 기반(구현 자체는 보통 서드파티)

한 줄 요약: **쿠버네티스 코어는 “컨트롤 플레인 + 표준 리소스 모델 + 확장 인터페이스(CNI/CSI/CRD)”**를 제공해요.

2) 대표적인 서드파티(3rd party) 솔루션(애드온/에코시스템)

쿠버네티스가 “인터페이스”만 제공하거나, 코어만으로는 운영이 불편한 영역을 채우는 것들입니다.

A. 네트워킹(CNI) 구현체

Calico, Cilium, Flannel, Weave Net 등

Pod 간 통신, NetworkPolicy enforcement, eBPF 기반 성능/가시성(특히 Cilium) 같은 “실제 기능”을 담당

B. Ingress / Gateway(외부 트래픽 진입)

NGINX Ingress Controller, HAProxy Ingress, Traefik

Gateway API 기반 솔루션(예: Envoy 기반 컨트롤러들)

K8s의 Ingress/Gateway “리소스”는 있어도, 실제 동작시키는 컨트롤러는 보통 서드파티입니다.

C. 서비스 메시(Service Mesh)

Istio, Linkerd, Kuma

mTLS, 트래픽 라우팅(카나리/AB), 정책, 관측(telemetry) 강화

D. 스토리지(CSI) / 데이터 서비스

Rook-Ceph, Longhorn, OpenEBS

클라우드 벤더 CSI(EBS/GCE PD/Azure Disk 등)

PV/PVC는 코어지만 “어디에 어떤 스토리지를 만들지”는 CSI가 구현

E. 관측(Observability)

Prometheus, Grafana(메트릭)

Loki / ELK(OpenSearch) / Fluent Bit/Fluentd(로그)

Jaeger / Tempo(트레이싱)

OpenTelemetry(수집 표준)

F. 배포/릴리즈/구성 관리(GitOps 포함)

Helm(패키징/템플릿)

Argo CD / Flux(GitOps)

Argo Rollouts / Flagger(카나리/블루그린 고급 배포)

G. 보안/정책/컴플라이언스

OPA Gatekeeper / Kyverno(정책-as-code)

cert-manager(TLS 인증서 자동화)

Vault / External Secrets Operator(시크릿 외부 연동)

Falco(런타임 위협 탐지)

H. 백업/DR

Velero(클러스터/리소스/PV 백업 & 복구)

I. 오토스케일링 고도화

HPA는 코어, 하지만

KEDA(이벤트/큐 기반 스케일)

Cluster Autoscaler / Karpenter(노드 자동 확장)

J. 운영 플랫폼/배포판/관리형 서비스(“쿠버네티스 + 제품화”)

Rancher, OpenShift(플랫폼)

EKS/GKE/AKS(관리형 K8s)

빠른 구분 기준(실무 팁)

“쿠버네티스 오브젝트/컨트롤 플레인만으로 되는가?” → 코어

“컨트롤러/플러그인/에이전트를 추가 설치해야 실제로 동작하는가?” → 보통 서드파티

대표: CNI/CSI, Ingress Controller, Service Mesh, GitOps, Observability, Policy 엔진

원하면, 지금 사용 중인 환경 기준(예: k3s/k8s, 온프렘/클라우드, 필요한 기능: Ingress/관측/보안 등)으로 **“최소 구성(MVP) + 추천 서드파티 조합”**도 같이 정리해줄게요.

helm 의 역할

Helm은 쿠버네티스에서 **“애플리케이션 설치/업그레이드/롤백을 패키지처럼 관리”**하게 해주는 패키지 매니저입니다. (리눅스의 apt, yum 같은 포지션)

Helm이 하는 일(역할)
1) Kubernetes 매니페스트 패키징(Chart)

Deployment, Service, Ingress, ConfigMap, HPA 같은 YAML들을 **하나의 묶음(Chart)**으로 구성

앱을 배포할 때 필요한 리소스들을 “세트 메뉴”처럼 제공

2) 템플릿화 & 환경별 값 분리

YAML을 그대로 복붙하지 않고, **변수(Values)**로 환경 차이를 분리

예: dev/prod에서 replicaCount, image.tag, ingress.host, resources만 다르게

같은 Chart를 values 파일만 바꿔서 재사용 가능

3) 릴리즈 단위 관리

Helm은 설치 결과를 **Release(릴리즈)**라는 단위로 관리

그래서

helm install (설치)

helm upgrade (업그레이드)

helm rollback (롤백)

helm uninstall (삭제)

helm history (배포 이력)
이게 깔끔하게 됩니다.

4) 의존성(Dependency) 관리

“이 앱은 Redis/DB/Ingress Controller가 필요” 같은 걸 chart dependency로 선언 가능

앱 스택을 함께 설치하기 쉬움

5) 표준 배포 방식 제공(운영 편의)

팀 내 배포 표준을 Chart로 만들어두면

신규 환경 구축/재현이 빨라지고

매니페스트 관리가 정돈되고

변경/배포 이력이 추적됨

Helm이 “안 하는 것 / 오해 포인트”

Helm 자체는 클러스터 상태를 계속 감시하며 자동으로 ‘원하는 상태’를 맞추는 GitOps 도구가 아님

그 역할은 보통 Argo CD / Flux가 담당(지속 동기화)

Helm은 “명령 실행 시점에 템플릿 렌더링 → 리소스 생성/업데이트”가 핵심

한 줄 요약

Helm = 쿠버네티스 YAML들을 템플릿 + 값(values)로 관리해서, 앱을 패키지(Chart)처럼 설치/업그레이드/롤백하는 도구입니다.

NAME                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes-dashboard-api               ClusterIP   10.43.11.189    <none>        8000/TCP   3h27m
service/kubernetes-dashboard-auth              ClusterIP   10.43.202.71    <none>        8000/TCP   3h27m
service/kubernetes-dashboard-kong-proxy        ClusterIP   10.43.84.207    <none>        443/TCP    3h27m
service/kubernetes-dashboard-metrics-scraper   ClusterIP   10.43.21.162    <none>        8000/TCP   3h27m
service/kubernetes-dashboard-web               ClusterIP   10.43.198.132   <none>        8000/TCP   3h27m 해당 리소스의 자세한 설명

좋아요. 지금 출력은 **Kubernetes Dashboard(신버전 계열: API/Web/Auth 분리 + Kong 프록시)**가 설치되면서 생성된 Service 리소스들이에요. 전부 ClusterIP라서 클러스터 내부에서만 접근 가능하고, 외부에서 보려면 kubectl port-forward나 Ingress/LoadBalancer를 추가로 붙여야 합니다.

아래는 각 서비스가 “무슨 역할을 하는지”를 기준으로 자세히 풀어쓴 설명입니다.

공통: TYPE=ClusterIP 의미

ClusterIP: 클러스터 내부 가상 IP(여기선 10.43.x.x 대역)를 하나 부여해서

Pod들이 서비스 이름(DNS) 또는 ClusterIP로 접근 가능

클러스터 밖에서는 직접 접근 불가

PORT(S)는 “서비스가 내부로 노출하는 포트”이고, 실제로는 selector로 연결된 Pod의 targetPort로 트래픽이 전달됩니다.

1) service/kubernetes-dashboard-web (8000/TCP)

대시보드 UI 프론트엔드(웹 앱) 역할입니다.

브라우저가 받는 웹 UI(HTML/JS/CSS) 쪽

사용자가 로그인/토큰 입력을 하면, 내부적으로 API 호출을 하게 되는데 그때 직접 API를 치는 게 아니라(구성에 따라) 보통 프록시를 경유하거나 내부 서비스로 연결됩니다.

포트 8000은 “웹 서비스가 수신하는 포트”

👉 내부 구성적으로 “사용자에게 보이는 화면” 담당

2) service/kubernetes-dashboard-api (8000/TCP)

대시보드의 백엔드 API 서버입니다.

UI가 요청하는 데이터(리소스 목록, Pod 로그, YAML 보기 등)를 처리

실제 클러스터 API 접근은 서비스어카운트 권한(RBAC)에 의해 통제됨

포트 8000

👉 “대시보드 기능의 핵심 백엔드”

3) service/kubernetes-dashboard-auth (8000/TCP)

인증/세션/로그인 처리 전담 컴포넌트입니다.

토큰/인증 흐름을 담당 (버전에 따라 OIDC 같은 외부 인증 연동도 이 영역에서 처리)

UI에서 로그인할 때 이쪽과 상호작용해서 인증 상태를 확립

포트 8000

👉 “로그인/인증 분리(보안 목적)”

4) service/kubernetes-dashboard-metrics-scraper (8000/TCP)

대시보드에서 ‘Metrics’ 화면을 위해 메트릭을 긁어오는 스크레이퍼입니다.

예전 대시보드는 metrics-scraper가 metrics-server로부터 리소스 사용량(CPU/Mem 등)을 가져와서 UI에 보여줬습니다.

즉, 이 서비스 자체가 메트릭 원천은 아니고,

원천: 보통 metrics-server(별도 설치) 또는 클러스터 메트릭 제공 컴포넌트

scraper: 그걸 “대시보드에서 쓰기 좋게” 중계/수집

포트 8000

👉 metrics-server가 없으면 대시보드에서 사용량 그래프/수치가 비거나 제한될 수 있어요.

5) service/kubernetes-dashboard-kong-proxy (443/TCP)

**Kong(게이트웨이/리버스 프록시) 기반의 단일 진입점(프론트 도어)**입니다.

443/TCP로 TLS(HTTPS) 트래픽을 받아서 내부의

dashboard-web

dashboard-api

dashboard-auth

(필요 시) metrics-scraper
로 라우팅해줍니다.

“대시보드 구성요소가 여러 개로 쪼개졌는데, 사용자는 한 주소로 접속”하게 만드는 역할

운영에서 외부 노출할 때 보통 이 kong-proxy를

NodePort로 바꾸거나

LoadBalancer로 바꾸거나

Ingress를 붙이는 식으로 사용합니다.

👉 쉽게 말해: “대시보드의 관문(Reverse Proxy/API Gateway)”

실제로 이 서비스들이 어떻게 연결됐는지 확인하는 명령

각 서비스가 어떤 Pod로 트래픽을 보내는지는 아래로 바로 확인 가능합니다.

# 서비스 상세(Selector, TargetPort, Endpoints 확인)
kubectl -n kubernetes-dashboard describe svc kubernetes-dashboard-web
kubectl -n kubernetes-dashboard describe svc kubernetes-dashboard-api
kubectl -n kubernetes-dashboard describe svc kubernetes-dashboard-auth
kubectl -n kubernetes-dashboard describe svc kubernetes-dashboard-metrics-scraper
kubectl -n kubernetes-dashboard describe svc kubernetes-dashboard-kong-proxy

# 엔드포인트(실제로 연결된 Pod IP:Port)
kubectl -n kubernetes-dashboard get endpoints

접근 방식(ClusterIP라 외부에서 바로 안 열림)

외부 브라우저에서 잠깐 확인만 하려면 보통:

kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443


그리고 로컬에서 https://localhost:8443로 접속합니다(인증서 경고는 날 수 있음).